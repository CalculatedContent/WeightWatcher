{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "2020-10-07T20:57:37-07:00\n",
      "\n",
      "CPython 3.7.4\n",
      "IPython 7.17.0\n",
      "\n",
      "compiler   : Clang 4.0.1 (tags/RELEASE_401/final)\n",
      "system     : Darwin\n",
      "release    : 17.7.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 12\n",
      "interpreter: 64bit\n",
      "\n",
      "\n",
      "tensorflow  2.1.0\n",
      "keras  2.2.4-tf\n",
      "torch  1.6.0\n"
     ]
    }
   ],
   "source": [
    "import sys, os \n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import torch\n",
    "\n",
    "#import tensorflow.keras.models.load_model\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark\n",
    "print(\"\\n\")\n",
    "print(\"tensorflow \",tf.__version__)\n",
    "print(\"keras \",keras.__version__)\n",
    "print(\"torch \",torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Test Models\n",
    "\n",
    "- pyTorch: vgg\n",
    "- keras:  vgg\n",
    "- huggingface: gpt_x\n",
    "- other ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# problem:  VERY OLD API\n",
    "\n",
    "# load vgg model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "# load the model\n",
    "vgg16_keras = VGG16()\n",
    "vgg19_keras = VGG19()\n",
    "\n",
    "# summarize the model\n",
    "# model.summary()\n",
    "\n",
    "#deprecated \n",
    "#from pytorchcv.model_provider import get_model as ptcv_get_model\n",
    "#vgg16_pytorch = ptcv_get_model('vgg16', pretrained=True)\n",
    "import torchvision\n",
    "vgg16_pytorch = torchvision.models.vgg16(pretrained=True)\n",
    "vgg16_pytorch_init = torchvision.models.vgg16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs\n",
    "\n",
    "https://keras.io/api/layers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from enum import IntFlag, auto, Enum\n",
    "\n",
    "class LAYER_TYPE(IntFlag):\n",
    "    UNKNOWN = auto()\n",
    "    DENSE = auto()\n",
    "    CONV1D = auto()\n",
    "    CONV2D = auto()\n",
    "    FLATTENED = auto()\n",
    "    EMBEDDING = auto()\n",
    "    NORM = auto()\n",
    "\n",
    "    \n",
    "class FRAMEWORK(IntFlag):\n",
    "    UNKNOWN = auto()\n",
    "    PYTORCH = auto()\n",
    "    KERAS = auto()\n",
    "    \n",
    "class FRAMEWORK(IntFlag):\n",
    "    UNKNOWN = auto()\n",
    "    PYTORCH = auto()\n",
    "    KERAS = auto()\n",
    "    \n",
    "class CHANNELS(IntFlag):\n",
    "    FIRST = auto()\n",
    "    LAST = auto()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can I extend / annotate the layer to add more elements\n",
    "\n",
    "- base class -> child ?  ugh\n",
    "- mixin\n",
    "- wrapper class that is a class (I forget the pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class WWLayer:\n",
    "    \"\"\"WW wrapper layer to Keras / PyTorch Layer object\"\"\"\n",
    "    def __init__(self, layer, index=-1, name=\"\", \n",
    "                 the_type=LAYER_TYPE.UNKNOWN, framework=FRAMEWORK.UNKNOWN, skipped=False):\n",
    "        self.layer = layer\n",
    "        self.index = index\n",
    "        self.name = name\n",
    "        self.skipped = skipped\n",
    "        self.the_type = the_type\n",
    "        self.framework = framework\n",
    "        \n",
    "        if (self.framework==FRAMEWORK.KERAS):\n",
    "             self.channels = CHANNELS.FIRST\n",
    "        elif (self.framework==FRAMEWORK.PYTORCH):\n",
    "            self.channels = CHANNELS.LAST\n",
    "        \n",
    "        # orginal weights and biases\n",
    "        self.has_weights = False\n",
    "        self.weights = None\n",
    "        \n",
    "        self.has_biases = False\n",
    "        self.biases = None\n",
    "        \n",
    "        # extracted weight matrices\n",
    "        self.num_W = 0\n",
    "        self.Wmats = []\n",
    "        self.N = 0\n",
    "        self.M = 0\n",
    "        self.num_components = self.M\n",
    "        self.rf = 1 # receptive field size\n",
    "        self.inputs_shape = []\n",
    "        self.outputs_shape = []\n",
    "        \n",
    "        # evals\n",
    "        self.evals = None\n",
    "        \n",
    "        # details\n",
    "        self.columns = []\n",
    "        \n",
    "        \n",
    "    def add_column(self, name, value):\n",
    "        \"\"\"Add column to the details dataframe\"\"\"\n",
    "        self.columns.append(name)\n",
    "        self.__dict__[name] =  value\n",
    "            \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"WWLayer()\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"WWLayer {}  {} {} {}  skipped {}\".format(self.index, self.name, \n",
    "                                                       self.framework.name, self.the_type.name, self.skipped)\n",
    "        \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_iter(model):\n",
    "    layer_iter = None\n",
    "    \n",
    "    if hasattr(model, 'layers'):\n",
    "        layer_iter = (l for l in model.layers)\n",
    "        framework = FRAMEWORK.KERAS\n",
    "    elif hasattr(model, 'modules'):\n",
    "        layer_iter = model.modules()\n",
    "        framework = FRAMEWORK.PYTORCH\n",
    "    else:\n",
    "        layer_iter = None\n",
    "        framework = FRAMEWORK.UNKNOWN\n",
    "        \n",
    "    return layer_iter, framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def layer_weights(ww_layer):\n",
    "    \"\"\"obtain the orginal weights and biases for the layer, if present\"\"\"\n",
    "    \n",
    "    has_weights, has_biases = False, False\n",
    "    weights, biases = None, None\n",
    "    rf = 1 # default for dense layer\n",
    "    \n",
    "    l = ww_layer.layer\n",
    "      \n",
    "    if ww_layer.framework==FRAMEWORK.PYTORCH:\n",
    "        if hasattr(l, 'weight'):\n",
    "            w = [np.array(l.weight.data.clone().cpu())]\n",
    "            has_weights = True\n",
    "            \n",
    "    elif ww_layer.framework==FRAMEWORK.KERAS:\n",
    "        w = l.get_weights()\n",
    "        if(len(w)>0):\n",
    "            has_weights = True\n",
    "            \n",
    "        if(len(w)>1):\n",
    "            has_biases = True\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        print(\"unknown framework\")\n",
    "        assert(False)\n",
    "   \n",
    "    if has_weights:\n",
    "        if len(w)==1:\n",
    "            weights = w[0]\n",
    "            biases = None\n",
    "        elif len(w)==2:\n",
    "            weights = w[0]\n",
    "            biases =  w[1]\n",
    "        else:\n",
    "            print(\"unknown weight shape\")\n",
    "            assert(False)\n",
    "        \n",
    "    \n",
    "    #receptive_field_size = l.weight.data[0][0].numel()\n",
    "    #            else:\n",
    "    \n",
    "    return has_weights, weights, has_biases, biases        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " \n",
    "# TODO:  maybe change to use channels.first / channels.last \n",
    "\n",
    "def get_conv2D_Wmats(Wtensor):\n",
    "        \"\"\"Extract W slices from a 4 index conv2D tensor of shape: (N,M,i,j) or (M,N,i,j).  \n",
    "        Return ij (N x M) matrices\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.debug(\"get_conv2D_Wmats\")\n",
    "\n",
    "        Wmats = []\n",
    "        s = Wtensor.shape\n",
    "        N, M, imax, jmax = s[0],s[1],s[2],s[3]\n",
    "        if N + M >= imax + jmax:\n",
    "            self.debug(\"Pytorch tensor shape detected: {}x{} (NxM), {}x{} (i,j)\".format(N, M, imax, jmax))\n",
    "            \n",
    "            for i in range(imax):\n",
    "                for j in range(jmax):\n",
    "                    W = Wtensor[:,:,i,j]\n",
    "                    if N < M:\n",
    "                        W = W.T\n",
    "                    Wmats.append(W)\n",
    "        else:\n",
    "            N, M, imax, jmax = imax, jmax, N, M          \n",
    "            self.debug(\"Tf.Keras.tensor shape detected: {}x{} (NxM), {}x{} (i,j)\".format(N, M, imax, jmax))\n",
    "            \n",
    "            for i in range(imax):\n",
    "                for j in range(jmax):\n",
    "                    W = Wtensor[i,j,:,:]\n",
    "                    if N < M:\n",
    "                        W = W.T\n",
    "                    Wmats.append(W)\n",
    "                    \n",
    "        self.debug(\"get_conv2D_Wmats N={} M={}\".format(N,M))\n",
    "\n",
    "        rf = imax*jmax # receptive field size \n",
    "        return Wmats, N, M, rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def set_weight_matrices(layer, conv2d_fft=False, conv2d_norm=True):\n",
    "    \"\"\"extract the weight matrices from the layer weights (tensors)\n",
    "    sets the weights on the layer\n",
    "    \n",
    "        conv2d_fft not supported yet \"\"\"\n",
    "    \n",
    "    if not layer.has_weights:\n",
    "        # throw exception ?\n",
    "        return \n",
    "        \n",
    "    weights = layer.weights\n",
    "    the_type = layer.the_type\n",
    "    N, M, n_comp, rf = 0, 0, 0, None\n",
    "    Wmats = []\n",
    "    \n",
    "    # this may change if we treat Conv1D differently layer\n",
    "    if (the_type == LAYER_TYPE.DENSE or the_type == LAYER_TYPE.CONV1D):\n",
    "        Wmats = [weights]\n",
    "        N, M = np.max(weights.shape), np.min(weights.shape)\n",
    "        n_comp = M\n",
    "        rf = 1\n",
    "        \n",
    "    elif the_type == LAYER_TYPE.CONV2D:\n",
    "        Wmats, N, M, rf = get_conv2D_Wmats(weights)\n",
    "        n_comp = M\n",
    "        \n",
    "    elif the_type == LAYER_TYPE.NORM:\n",
    "        pass#print(\"Layer norm has no matrices\")\n",
    "    \n",
    "    else:\n",
    "        print(\"unknown type {} layer  {}\".format(the_type, layer.layer))\n",
    "\n",
    "    layer.N = N\n",
    "    layer.M = M\n",
    "    layer.rf = rf\n",
    "    layer.Wmats = Wmats\n",
    "    layer.num_components = n_comp\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_type_list(filename):\n",
    "    \"\"\"Read a list of layer types\"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def layer_type(layer):\n",
    "    \"\"\"Detemine layer type\"\"\"\n",
    "    \n",
    "    the_type = LAYER_TYPE.UNKNOWN\n",
    "    typestr = (str(type(layer))).lower()\n",
    "        \n",
    "    # Keras TF 2.x types\n",
    "    if isinstance(layer, tf.keras.layers.Dense): \n",
    "        the_type = LAYER_TYPE.DENSE\n",
    "        \n",
    "    elif isinstance(layer, keras.layers.Conv1D):                \n",
    "        the_type = LAYER_TYPE.CONV1D\n",
    "    \n",
    "    elif isinstance(layer, keras.layers.Conv2D):                \n",
    "        the_type = LAYER_TYPE.CONV2D\n",
    "        \n",
    "    elif isinstance(layer, tf.keras.layers.Flatten):\n",
    "        the_type = LAYER_TYPE.FLATTENED\n",
    "        \n",
    "    elif isinstance(layer, tf.keras.layers.Embedding):\n",
    "        the_type = LAYER_TYPE.EMBEDDING\n",
    "        \n",
    "    elif isinstance(layer, tf.keras.layers.LayerNormalization):\n",
    "        the_type = LAYER_TYPE.NORM\n",
    "        \n",
    "        \n",
    " \n",
    "    # PyTorch\n",
    "    \n",
    "         \n",
    "    elif isinstance(layer, torch.nn.Linear):\n",
    "        the_type = LAYER_TYPE.DENSE\n",
    "        \n",
    "    elif isinstance(layer, torch.nn.Conv1d):\n",
    "        the_type = LAYER_TYPE.CONV1D\n",
    "\n",
    "    elif isinstance(layer, torch.nn.Conv2d):\n",
    "        the_type = LAYER_TYPE.CONV2D\n",
    "        \n",
    "    elif isinstance(layer, torch.nn.Embedding):\n",
    "        the_type = LAYER_TYPE.EMBEDDING\n",
    "            \n",
    "    elif isinstance(layer, torch.nn.LayerNorm):\n",
    "        the_type = LAYER_TYPE.NORM\n",
    "        \n",
    "\n",
    "    # try to infer type (i.e for huggingface)\n",
    "    elif typestr.endswith(\".linear'>\"):\n",
    "        the_type = LAYER_TYPE.DENSE\n",
    "        \n",
    "    elif typestr.endswith(\".dense'>\"):\n",
    "        the_type = LAYER_TYPE.DENSE\n",
    "        \n",
    "    elif typestr.endswith(\".conv1d'>\"):\n",
    "        the_type = LAYER_TYPE.CONV1D\n",
    "        \n",
    "    elif typestr.endswith(\".conv2d'>\"):\n",
    "        the_type = LAYER_TYPE.CONV2D\n",
    "\n",
    "    return the_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_ww_layer(layer, index, filter_ids=None, filter_types=None, framework=None):\n",
    "    \"\"\"Make ww_layer or return None if layer is skipped\"\"\"\n",
    "    \n",
    "    skipped = False\n",
    "    the_type = layer_type(layer)\n",
    "    name = \"\"\n",
    "    has_weights = False;\n",
    "    \n",
    "    if hasattr(layer, 'name'):\n",
    "        name = layer.name\n",
    "    \n",
    "    if filter_ids is not None and len(filter_ids) > 0:\n",
    "        if layer_id not in filter_ids:\n",
    "            skipped = True\n",
    "            \n",
    "    if filter_types is not None and len(filter_types) > 0:\n",
    "        if the_type not in filter_types:\n",
    "            skipped = True\n",
    "                                    \n",
    "    \n",
    "    ww_layer = WWLayer(layer, index=index, name=name, \n",
    "                 the_type=the_type, framework=framework, skipped=skipped)\n",
    "    \n",
    "    has_weights, weights, has_biases, biases = layer_weights(ww_layer)\n",
    "    \n",
    "    \n",
    "    ww_layer.has_weights = has_weights\n",
    "    ww_layer.has_biases = has_biases\n",
    "    \n",
    "    if has_biases:\n",
    "        ww_layer.biases = biases   \n",
    "        \n",
    "    if has_weights:    \n",
    "        ww_layer.weights = weights\n",
    "        set_weight_matrices(ww_layer)\n",
    "\n",
    "    \n",
    "    return ww_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class WWLayerIterator:\n",
    "\n",
    "    \"\"\"Iterator that loops over layers, with matrices available.\"\"\"\n",
    "\n",
    "    def __init__(self, model, filter_ids=[], filter_types=[]):\n",
    "        self.model = model\n",
    "        self.layers_iter, self.framework = model_iter(model) \n",
    "\n",
    "        self.filter_ids = filter_ids\n",
    "        self.filter_types = filter_types\n",
    "        \n",
    "        self.k = 0\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    # Python 3 compatibility\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        curr_layer = next(self.layers_iter)\n",
    "        if curr_layer:    \n",
    "            curr_id, self.k = self.k, self.k+1\n",
    "            #curr_layer = self.layers[curr_id]\n",
    "            ww_layer = make_ww_layer(curr_layer, curr_id, \n",
    "                                     filter_ids=self.filter_ids, \n",
    "                                     filter_types=self.filter_types, \n",
    "                                     framework=self.framework)\n",
    "                        \n",
    "            return ww_layer\n",
    "        else:\n",
    "            raise StopIteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with different frameworks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WWLayer 0  input_1 KERAS UNKNOWN  skipped False\n",
      "W : (3, 3, 3, 64)\n",
      "b  (64,)\n",
      ".......\n",
      "W : (3, 3, 64, 64)\n",
      "b  (64,)\n",
      ".......\n",
      ".......\n",
      "W : (3, 3, 64, 128)\n",
      "b  (128,)\n",
      ".......\n",
      "W : (3, 3, 128, 128)\n",
      "b  (128,)\n",
      ".......\n",
      ".......\n",
      "W : (3, 3, 128, 256)\n",
      "b  (256,)\n",
      ".......\n",
      "W : (3, 3, 256, 256)\n",
      "b  (256,)\n",
      ".......\n",
      "W : (3, 3, 256, 256)\n",
      "b  (256,)\n",
      ".......\n",
      ".......\n",
      "W : (3, 3, 256, 512)\n",
      "b  (512,)\n",
      ".......\n",
      "W : (3, 3, 512, 512)\n",
      "b  (512,)\n",
      ".......\n",
      "W : (3, 3, 512, 512)\n",
      "b  (512,)\n",
      ".......\n",
      ".......\n",
      "W : (3, 3, 512, 512)\n",
      "b  (512,)\n",
      ".......\n",
      "W : (3, 3, 512, 512)\n",
      "b  (512,)\n",
      ".......\n",
      "W : (3, 3, 512, 512)\n",
      "b  (512,)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (25088, 4096)\n",
      "b  (4096,)\n",
      ".......\n",
      "W : (4096, 4096)\n",
      "b  (4096,)\n",
      ".......\n",
      "W : (4096, 1000)\n",
      "b  (1000,)\n",
      ".......\n"
     ]
    }
   ],
   "source": [
    "ww_layer_iterator = WWLayerIterator(vgg16_keras)\n",
    "\n",
    "for ww_layer in ww_layer_iterator:\n",
    "    print(ww_layer)\n",
    "    for ww_layer in ww_layer_iterator:\n",
    "    #    print(ww_layer)\n",
    "        if(ww_layer.has_weights):\n",
    "            print(\"W :\",ww_layer.weights.shape)\n",
    "        if(ww_layer.has_biases):\n",
    "            print(\"b \",ww_layer.biases.shape)\n",
    "        print(\".......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WWLayer 0   PYTORCH UNKNOWN  skipped False\n",
      ".......\n",
      "W : (64, 3, 3, 3)\n",
      ".......\n",
      ".......\n",
      "W : (64, 64, 3, 3)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (128, 64, 3, 3)\n",
      ".......\n",
      ".......\n",
      "W : (128, 128, 3, 3)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (256, 128, 3, 3)\n",
      ".......\n",
      ".......\n",
      "W : (256, 256, 3, 3)\n",
      ".......\n",
      ".......\n",
      "W : (256, 256, 3, 3)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (512, 256, 3, 3)\n",
      ".......\n",
      ".......\n",
      "W : (512, 512, 3, 3)\n",
      ".......\n",
      ".......\n",
      "W : (512, 512, 3, 3)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (512, 512, 3, 3)\n",
      ".......\n",
      ".......\n",
      "W : (512, 512, 3, 3)\n",
      ".......\n",
      ".......\n",
      "W : (512, 512, 3, 3)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (4096, 25088)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (4096, 4096)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (1000, 4096)\n",
      ".......\n"
     ]
    }
   ],
   "source": [
    "ww_layer_iterator = WWLayerIterator(vgg16_pytorch)\n",
    "\n",
    "for ww_layer in ww_layer_iterator:\n",
    "    print(ww_layer)\n",
    "    for ww_layer in ww_layer_iterator:\n",
    "    #    print(ww_layer)\n",
    "        if(ww_layer.has_weights):\n",
    "            print(\"W :\",ww_layer.weights.shape)\n",
    "        if(ww_layer.has_biases):\n",
    "            print(\"b \",ww_layer.biases.shape)\n",
    "        print(\".......\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      ".......\n",
      "unknown type 32 layer  Embedding(50257, 768)\n",
      "W : (50257, 768)\n",
      ".......\n",
      "unknown type 32 layer  Embedding(1024, 768)\n",
      "W : (1024, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 2304)\n",
      ".......\n",
      "W : (768, 768)\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      ".......\n",
      "W : (768, 3072)\n",
      ".......\n",
      "W : (3072, 768)\n",
      ".......\n",
      ".......\n",
      "W : (768,)\n",
      ".......\n",
      "W : (50257, 768)\n",
      ".......\n"
     ]
    }
   ],
   "source": [
    "from transformers import  GPT2LMHeadModel\n",
    "gpt2_lmh = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "ww_layer_iterator = WWLayerIterator(gpt2_lmh)\n",
    "for ww_layer in ww_layer_iterator:\n",
    "#    print(ww_layer)\n",
    "    if(ww_layer.has_weights):\n",
    "        print(\"W :\",ww_layer.weights.shape)\n",
    "    if(ww_layer.has_biases):\n",
    "        print(\"b \",ww_layer.biases.shape)\n",
    "    print(\".......\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3, 64) (64,)\n",
      "(3, 3, 64, 64) (64,)\n",
      "(3, 3, 64, 128) (128,)\n",
      "(3, 3, 128, 128) (128,)\n",
      "(3, 3, 128, 256) (256,)\n",
      "(3, 3, 256, 256) (256,)\n",
      "(3, 3, 256, 256) (256,)\n",
      "(3, 3, 256, 512) (512,)\n",
      "(3, 3, 512, 512) (512,)\n",
      "(3, 3, 512, 512) (512,)\n",
      "(3, 3, 512, 512) (512,)\n",
      "(3, 3, 512, 512) (512,)\n",
      "(3, 3, 512, 512) (512,)\n",
      "(25088, 4096) (4096,)\n",
      "(4096, 4096) (4096,)\n",
      "(4096, 1000) (1000,)\n"
     ]
    }
   ],
   "source": [
    "layer_iterator = WWLayerIterator(vgg16_keras)\n",
    "\n",
    "for ww_layer in layer_iterator:\n",
    "    if(ww_layer.has_weights):\n",
    "        print(ww_layer.weights.shape, ww_layer.biases.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- set a few basics for keras and tf, then infer the rest by name\n",
    "- allow a yaml that people can edit with lists of others\n",
    "\n",
    "\n",
    "\n",
    "- set rf, M, N\n",
    "- set orig tensor ?\n",
    "- be able to reset weights => layer iterator allows get and set\n",
    "- try to get inputs , outputs for both frameworks\n",
    "- test attention model => need to label properly the attention layer matrices\n",
    "- extract combined eigenvalues for Conv2D\n",
    "- extract matrices for attention\n",
    "- RNN, LSTM, ... (pretrained from where)\n",
    "- be able to take W-Wdiff using original tensor, not slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:  Wednesday++\n",
    "\n",
    "- write wwcomparator: use iterator to compare |W-Winit|\n",
    "- write wwregularizer:  use iterator to compute SVD, output new model\n",
    "- rewrite ww:  compute SVD, ESD, supporting metrics, etc\n",
    "\n",
    "#### Python Lint s\n",
    "- clean nup documentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# not used\n",
    "\n",
    "def same_models(model_1, model_2):\n",
    "    \"\"\"Compare models to see if the are the same architecture\"\"\"\n",
    "    \n",
    "    same = True\n",
    "    layer_iter_1 = WWLayerIterator(model_1)\n",
    "    layer_iter_2 = WWLayerIterator(model_2)\n",
    "    \n",
    "    same = layer_iter_1.framework == layer_iter_2.framework \n",
    "\n",
    "  \n",
    "    return same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def distances(model_1, model_2):\n",
    "    \"\"\"Compute the distances between model_1 and model_2 for each layer. \n",
    "    Reports Frobenius norm of the distance between each layer weights (tensor)\n",
    "    \n",
    "       < ||W_1-W_2|| >\n",
    "       \n",
    "    output: avg delta W, a details dataframe\n",
    "       \n",
    "    models should be the same size and from the same framework\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    # check and throw exception if inputs incorrect\n",
    "    # TODO: review design here...may need something else\n",
    "    #   need to:\n",
    "    #.   - iterate over all layers and check\n",
    "    #.   - inspect framework by framework\n",
    "    #.   - check here instead\n",
    "    #\n",
    "    \n",
    "    same = True\n",
    "    layer_iter_1 = WWLayerIterator(model_1)\n",
    "    layer_iter_2 = WWLayerIterator(model_2)\n",
    "    \n",
    "    same = layer_iter_1.framework == layer_iter_2.framework \n",
    "    if not same:\n",
    "        raise Exception(\"Sorry, models are from different frameworks\")\n",
    "        \n",
    "   \n",
    "    \n",
    "    details = pd.DataFrame(columns = ['layer_id', 'name', 'delta_W', 'delta_b', 'W_shape', 'b_shape'])\n",
    "    data = {}\n",
    "    \n",
    "    try:      \n",
    "        for layer_1, layer_2 in zip(layer_iter_1, layer_iter_2):\n",
    "            data['layer_id'] = layer_1.index\n",
    "            data['name'] = layer_1.name\n",
    "\n",
    "            if layer_1.has_weights:\n",
    "                data['delta_W'] = np.linalg.norm(layer_1.weights-layer_2.weights)\n",
    "                data['W_shape'] = layer_1.weights.shape\n",
    "\n",
    "                if layer_1.has_biases:\n",
    "                    data['delta_b'] = np.linalg.norm(layer_1.biases-layer_2.biases)\n",
    "                    data['b_shape'] = layer_1.biases.shape\n",
    "\n",
    "                details = details.append(data,  ignore_index=True)\n",
    "    except:\n",
    "        raise Exception(\"Sorry, problem comparing models\")\n",
    "    \n",
    "    details.set_index('layer_id', inplace=True)\n",
    "    avg_dW = np.mean(details['delta_W'].to_numpy())\n",
    "    return avg_dW, details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>delta_W</th>\n",
       "      <th>delta_b</th>\n",
       "      <th>W_shape</th>\n",
       "      <th>b_shape</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>block1_conv1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(3, 3, 3, 64)</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>block1_conv2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(3, 3, 64, 64)</td>\n",
       "      <td>(64,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>block2_conv1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(3, 3, 64, 128)</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>block2_conv2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(3, 3, 128, 128)</td>\n",
       "      <td>(128,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>block3_conv1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(3, 3, 128, 256)</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>block3_conv2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(3, 3, 256, 256)</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>block3_conv3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(3, 3, 256, 256)</td>\n",
       "      <td>(256,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>block4_conv1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(3, 3, 256, 512)</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>block4_conv2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(3, 3, 512, 512)</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>block4_conv3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(3, 3, 512, 512)</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>block5_conv1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(3, 3, 512, 512)</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>block5_conv2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(3, 3, 512, 512)</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>block5_conv3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(3, 3, 512, 512)</td>\n",
       "      <td>(512,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>fc1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(25088, 4096)</td>\n",
       "      <td>(4096,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>fc2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(4096, 4096)</td>\n",
       "      <td>(4096,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>predictions</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(4096, 1000)</td>\n",
       "      <td>(1000,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  delta_W  delta_b           W_shape  b_shape\n",
       "layer_id                                                           \n",
       "1         block1_conv1      0.0      0.0     (3, 3, 3, 64)    (64,)\n",
       "2         block1_conv2      0.0      0.0    (3, 3, 64, 64)    (64,)\n",
       "4         block2_conv1      0.0      0.0   (3, 3, 64, 128)   (128,)\n",
       "5         block2_conv2      0.0      0.0  (3, 3, 128, 128)   (128,)\n",
       "7         block3_conv1      0.0      0.0  (3, 3, 128, 256)   (256,)\n",
       "8         block3_conv2      0.0      0.0  (3, 3, 256, 256)   (256,)\n",
       "9         block3_conv3      0.0      0.0  (3, 3, 256, 256)   (256,)\n",
       "11        block4_conv1      0.0      0.0  (3, 3, 256, 512)   (512,)\n",
       "12        block4_conv2      0.0      0.0  (3, 3, 512, 512)   (512,)\n",
       "13        block4_conv3      0.0      0.0  (3, 3, 512, 512)   (512,)\n",
       "15        block5_conv1      0.0      0.0  (3, 3, 512, 512)   (512,)\n",
       "16        block5_conv2      0.0      0.0  (3, 3, 512, 512)   (512,)\n",
       "17        block5_conv3      0.0      0.0  (3, 3, 512, 512)   (512,)\n",
       "20                 fc1      0.0      0.0     (25088, 4096)  (4096,)\n",
       "21                 fc2      0.0      0.0      (4096, 4096)  (4096,)\n",
       "22         predictions      0.0      0.0      (4096, 1000)  (1000,)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_dW, details = distances(vgg16_keras, vgg16_keras)\n",
    "details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>delta_W</th>\n",
       "      <th>delta_b</th>\n",
       "      <th>W_shape</th>\n",
       "      <th>b_shape</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>10.790011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(64, 3, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>15.694499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(64, 64, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>17.584372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(128, 64, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>21.841988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(128, 128, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>23.593069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(256, 128, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>29.470362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td>29.912214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(256, 256, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td></td>\n",
       "      <td>32.590324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(512, 256, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td></td>\n",
       "      <td>40.711391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td></td>\n",
       "      <td>40.681431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td></td>\n",
       "      <td>41.605820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td></td>\n",
       "      <td>41.340450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td></td>\n",
       "      <td>40.438801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(512, 512, 3, 3)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td></td>\n",
       "      <td>113.572182</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(4096, 25088)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td></td>\n",
       "      <td>57.999268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(4096, 4096)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td></td>\n",
       "      <td>42.499958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(1000, 4096)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name     delta_W  delta_b           W_shape  b_shape\n",
       "layer_id                                                     \n",
       "2               10.790011      NaN     (64, 3, 3, 3)      NaN\n",
       "4               15.694499      NaN    (64, 64, 3, 3)      NaN\n",
       "7               17.584372      NaN   (128, 64, 3, 3)      NaN\n",
       "9               21.841988      NaN  (128, 128, 3, 3)      NaN\n",
       "12              23.593069      NaN  (256, 128, 3, 3)      NaN\n",
       "14              29.470362      NaN  (256, 256, 3, 3)      NaN\n",
       "16              29.912214      NaN  (256, 256, 3, 3)      NaN\n",
       "19              32.590324      NaN  (512, 256, 3, 3)      NaN\n",
       "21              40.711391      NaN  (512, 512, 3, 3)      NaN\n",
       "23              40.681431      NaN  (512, 512, 3, 3)      NaN\n",
       "26              41.605820      NaN  (512, 512, 3, 3)      NaN\n",
       "28              41.340450      NaN  (512, 512, 3, 3)      NaN\n",
       "30              40.438801      NaN  (512, 512, 3, 3)      NaN\n",
       "35             113.572182      NaN     (4096, 25088)      NaN\n",
       "38              57.999268      NaN      (4096, 4096)      NaN\n",
       "41              42.499958      NaN      (1000, 4096)      NaN"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_dW, details = distances(vgg16_pytorch, vgg16_pytorch_init)\n",
    "details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorator\n",
    "\n",
    "https://www.programiz.com/python-programming/decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make_ww_layer()\n",
    "\n",
    "Can I decorate an iterator ?\n",
    "\n",
    "Or just make another iterator\n",
    "\n",
    "- WWLayerSVDIterator\n",
    "- WWLayerESDIterator\n",
    "\n",
    "thats fine\n",
    "\n",
    "what about classes / objects ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Foo:\n",
    "    def __init__(self):\n",
    "        # we can dynamically have access to the properties dict using __dict__\n",
    "        self.__dict__['x'] = 'bar'\n",
    "        self.columns = []\n",
    "    \n",
    "    def add_column(self, name, value):\n",
    "        self.columns.append(name)\n",
    "        self.__dict__[name] =  value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foo = Foo()\n",
    "foo.add_column(\"y\", 3.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4\n"
     ]
    }
   ],
   "source": [
    "print(foo.y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD and ESD methods\n",
    "\n",
    "#### How to treat ?\n",
    " - analyze parameters:  min size, max_size  ?\n",
    " \n",
    " params = {} \n",
    " \n",
    " - use monkey patching to store new values s\n",
    " -  also store new names so we \n",
    "  -   know what was added\n",
    "  -   can extract later for the detaild dataframe\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make this a static method ?        \n",
    "def combined_eigenvalues( weights, N, M, n_comp, params):\n",
    "    \"\"\"Compute the eigenvalues for all weights of the NxM weight matrices (N >= M), \n",
    "        combined into a single, sorted, numpy array\n",
    "\n",
    "        Applied normalization and glorot_fix if specified\n",
    "\n",
    "        Assumes an array of weights comes from a conv2D layer and applies conv2d_norm normalization by default\n",
    "\n",
    "        Also returns max singular value and rank_loss, needed for other calculations\n",
    "     \"\"\"\n",
    "\n",
    "    all_evals = []\n",
    "    max_sv = 0.0\n",
    "    rank_loss = 0\n",
    "\n",
    "    normalize =params['normalize']\n",
    "    glorot_fix =params['glorot_fix']\n",
    "    conv2d_norm =params['conv2d_norm']\n",
    "\n",
    "\n",
    "    count = len(weights)\n",
    "    for  W in weights:\n",
    "\n",
    "        Q=N/M\n",
    "        #check, checkTF = glorot_norm_check(W, N, M, count) \n",
    "\n",
    "        # assume receptive field size is count\n",
    "        if glorot_fix:\n",
    "            W = glorot_norm_fix(W, N, M, count)\n",
    "        elif conv2d_norm:\n",
    "            # probably never needed since we always fix for glorot\n",
    "            W = W * np.sqrt(count/2.0) \n",
    "\n",
    "        # SVD can be swapped out here\n",
    "        # svd = TruncatedSVD(n_components=M-1, n_iter=7, random_state=10)\n",
    "\n",
    "        W = W.astype(float)\n",
    "        #self.debug(\"Running full SVD:  W.shape={}  n_comp = {}\".format(W.shape, n_comp))\n",
    "        sv = np.linalg.svd(W, compute_uv=False)\n",
    "        sv = sv.flatten()\n",
    "        sv = np.sort(sv)[-n_comp:]\n",
    "        #if len(sv) > max_size:\n",
    "        #    #self.info(\"chosing {} singular values from {} \".format(max_size, len(sv)))\n",
    "        #    sv = np.random.choice(sv, size=max_size)\n",
    "\n",
    "        #sv = svd.singular_values_\n",
    "        evals = sv*sv\n",
    "        if normalize:\n",
    "            evals = evals/N\n",
    "\n",
    "        all_evals.extend(evals)\n",
    "\n",
    "        max_sv = np.max([max_sv, np.max(sv)])\n",
    "        max_ev = np.max(evals)\n",
    "        rank_loss = 0#rank_loss + self.calc_rank_loss(sv, M, max_ev)\n",
    "\n",
    "    return np.sort(np.array(all_evals)), max_sv, rank_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def debug(msg):\n",
    "    print(msg)\n",
    "    \n",
    "\n",
    "          \n",
    "def apply_esd(layer, params={}):\n",
    "    \"\"\"run full SVD on layer weight matrices, compute ESD, combine all,  and save to layer \"\"\"\n",
    "    \n",
    "    layer_id = layer.index\n",
    "    name = layer.name\n",
    "    the_type = layer.the_type\n",
    "     \n",
    "    M  = layer.M\n",
    "    N  = layer.N\n",
    "    rf = 1.0#layer.receptive_field_size\n",
    "    \n",
    "    min_size = params['min_size']\n",
    "    max_size = params['max_size']\n",
    "    \n",
    " \n",
    "    if not layer.has_weights:\n",
    "        debug(\"Layer {} {} has no weights\".format(layer_id, name))\n",
    "    \n",
    "    elif the_type is LAYER_TYPE.UNKNOWN:\n",
    "        debug(\"Layer {} {} type {} unknown\".format(layer_id, name, the_type))\n",
    "    \n",
    "    elif the_type in [LAYER_TYPE.FLATTENED, LAYER_TYPE.NORM]:\n",
    "        debug(\"Layer {} {} type {} not supported\".format(layer_id, name, the_type))\n",
    "    \n",
    "    elif  M*rf < min_size:\n",
    "        debug(\"Layer {} {}: size {} < {}\".format(layer_id, name, M*rf, min_size))\n",
    "              \n",
    "    elif  N > max_size:\n",
    "        debug(\"Layer {} {}: size {} > {}\".format(layer_id, name, N, max_size))\n",
    "    \n",
    "    elif the_type  in [LAYER_TYPE.DENSE, LAYER_TYPE.CONV1D, LAYER_TYPE.CONV2D]:\n",
    "        Wmats = layer.Wmats\n",
    "        n_comp = layer.num_components\n",
    "        \n",
    "        params = {}\n",
    "        # can skip min_size, max_size directly here if we want to\n",
    "        params['normalize'] = True\n",
    "        params['glorot_fix'] = False\n",
    "        params['conv2d_norm'] = True\n",
    "        \n",
    "        evals, sv_max, rank_loss = combined_eigenvalues(Wmats, N, M, n_comp, params)\n",
    "     \n",
    "        layer.evals = evals\n",
    "        layer.add_column(\"has_esd\",True)\n",
    "        layer.add_column(\"num_evals\",len(evals))\n",
    "        layer.add_column(\"sv_max\",sv_max)\n",
    "        layer.add_column(\"rank_loss\",rank_loss)\n",
    "        layer.add_column(\"lambda_max\",np.max(evals))\n",
    "        \n",
    "    return layer\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_plot_esd(layer, params={}):\n",
    "    evals = ww_layer.evals\n",
    "    name = ww_layer.name\n",
    "    plt.title(name)\n",
    "    plt.hist(np.log10(evals), bins=100)\n",
    "    plt.show()\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def apply_power_law_fit(layer, params={}):\n",
    "    # check that ESD has been computed\n",
    "    \n",
    "    # how render the plots\n",
    "    \n",
    "    # if evals are crazy large, use robust eatimator\n",
    "    #  sample, get alpha, repeat N times\n",
    "    \n",
    "    #if params['plot']\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def apply_marchenko_pastur_fit(layer, params={}):\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def apply_smooth_model(layer, params={}):\n",
    "    # \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def details_row(layer, params={}):\n",
    "    data = {}\n",
    "    \n",
    "    data['layer_id']=layer.index\n",
    "    data['name']=layer.name\n",
    "    data['N']=layer.N\n",
    "    data['M']=layer.M\n",
    "    #data['rf']=layer.rf\n",
    "    \n",
    "    for col in layer.columns:\n",
    "        data[col]=layer.__dict__[col]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layer_iterator = WWLayerIterator(vgg16_keras)\n",
    "\n",
    "params = {}\n",
    "params['min_size']=0\n",
    "params['max_size']=100000\n",
    "\n",
    "\n",
    "details = pd.DataFrame(columns = ['layer_id', 'name'])\n",
    "    \n",
    "for ww_layer in layer_iterator:\n",
    "    if(ww_layer.has_weights):\n",
    "        apply_esd(ww_layer, params)\n",
    "        #apply_plot_esd(ww_layer, params)\n",
    "        #apply_...\n",
    "        #apl\n",
    "        \n",
    "        data = details_row(ww_layer, params={})        \n",
    "        details = details.append(data,  ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fix receptive field size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_id</th>\n",
       "      <th>name</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>has_esd</th>\n",
       "      <th>lambda_max</th>\n",
       "      <th>num_evals</th>\n",
       "      <th>rank_loss</th>\n",
       "      <th>sv_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>block1_conv1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.652284</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.912432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>block1_conv2</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.184081</td>\n",
       "      <td>576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.432377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>block2_conv1</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.171351</td>\n",
       "      <td>576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.311567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>block2_conv2</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.038044</td>\n",
       "      <td>1152.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.206711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>block3_conv1</td>\n",
       "      <td>256.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.053563</td>\n",
       "      <td>1152.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.618417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>block3_conv2</td>\n",
       "      <td>256.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018282</td>\n",
       "      <td>2304.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.163351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>block3_conv3</td>\n",
       "      <td>256.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015852</td>\n",
       "      <td>2304.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.014459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>block4_conv1</td>\n",
       "      <td>512.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.014959</td>\n",
       "      <td>2304.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.956916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>block4_conv2</td>\n",
       "      <td>512.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007665</td>\n",
       "      <td>4608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.981068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13</td>\n",
       "      <td>block4_conv3</td>\n",
       "      <td>512.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009096</td>\n",
       "      <td>4608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.158061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>block5_conv1</td>\n",
       "      <td>512.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>4608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.722328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>block5_conv2</td>\n",
       "      <td>512.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010474</td>\n",
       "      <td>4608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.315725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17</td>\n",
       "      <td>block5_conv3</td>\n",
       "      <td>512.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>4608.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.885333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>fc1</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>25088.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.780120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21</td>\n",
       "      <td>fc2</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.976500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22</td>\n",
       "      <td>predictions</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>4096.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.102831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   layer_id          name       M        N  has_esd  lambda_max  num_evals  \\\n",
       "0         1  block1_conv1    64.0      3.0      1.0   11.652284       27.0   \n",
       "1         2  block1_conv2    64.0     64.0      1.0    0.184081      576.0   \n",
       "2         4  block2_conv1   128.0     64.0      1.0    0.171351      576.0   \n",
       "3         5  block2_conv2   128.0    128.0      1.0    0.038044     1152.0   \n",
       "4         7  block3_conv1   256.0    128.0      1.0    0.053563     1152.0   \n",
       "5         8  block3_conv2   256.0    256.0      1.0    0.018282     2304.0   \n",
       "6         9  block3_conv3   256.0    256.0      1.0    0.015852     2304.0   \n",
       "7        11  block4_conv1   512.0    256.0      1.0    0.014959     2304.0   \n",
       "8        12  block4_conv2   512.0    512.0      1.0    0.007665     4608.0   \n",
       "9        13  block4_conv3   512.0    512.0      1.0    0.009096     4608.0   \n",
       "10       15  block5_conv1   512.0    512.0      1.0    0.005794     4608.0   \n",
       "11       16  block5_conv2   512.0    512.0      1.0    0.010474     4608.0   \n",
       "12       17  block5_conv3   512.0    512.0      1.0    0.016260     4608.0   \n",
       "13       20           fc1  4096.0  25088.0      1.0    0.000126     4096.0   \n",
       "14       21           fc2  4096.0   4096.0      1.0    0.000954     4096.0   \n",
       "15       22   predictions  1000.0   4096.0      1.0    0.001080     1000.0   \n",
       "\n",
       "    rank_loss    sv_max  \n",
       "0         0.0  5.912432  \n",
       "1         0.0  3.432377  \n",
       "2         0.0  3.311567  \n",
       "3         0.0  2.206711  \n",
       "4         0.0  2.618417  \n",
       "5         0.0  2.163351  \n",
       "6         0.0  2.014459  \n",
       "7         0.0  1.956916  \n",
       "8         0.0  1.981068  \n",
       "9         0.0  2.158061  \n",
       "10        0.0  1.722328  \n",
       "11        0.0  2.315725  \n",
       "12        0.0  2.885333  \n",
       "13        0.0  1.780120  \n",
       "14        0.0  1.976500  \n",
       "15        0.0  2.102831  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'min_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-14557cb673ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'min_size'"
     ]
    }
   ],
   "source": [
    "params= {}\n",
    "params['min_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
